
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Regression &mdash; Procedural Gaussian Processes 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Procedural Gaussian Processes 1.0 documentation" href="index.html" />
    <link rel="up" title="Demos" href="Examples.html" />
    <link rel="next" title="Classification Demonstration Example" href="demoClassification.html" />
    <link rel="prev" title="Demos" href="Examples.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="demoClassification.html" title="Classification Demonstration Example"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="Examples.html" title="Demos"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Procedural Gaussian Processes 1.0 documentation</a> &raquo;</li>
          <li><a href="GettingStarted.html" >Getting Started</a> &raquo;</li>
          <li><a href="Examples.html" accesskey="U">Demos</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="regression">
<h1>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h1>
<p>This example exactly recreates the regression example from the <a class="reference external" href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML</a> package.</p>
<p>This is a simple example, where <span class="math">\(n=20\)</span> data points are from a Gaussian Process (GP). The inputs
are scalar (so plotting is easy). We then use various other GPs to make inferences about the underlying function.</p>
<p>First, generate the exact data from the GPML example (this data is hardcoded in data/regression_data.npz).</p>
<p>cFirst specify the mean function meanfunc, covariance function covfunc of a GP and a likelihood function, likfunc. The
corresponding hyperparameters are specified in the <tt class="xref py py-class docutils literal"><span class="pre">src.Tools.utils.hyperParameters</span></tt> class.</p>
<p>The mean function is composite, adding (using <a class="reference internal" href="meanAPI.html#src.Core.means.meanSum" title="src.Core.means.meanSum"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.means.meanSum()</span></tt></a> function) a linear (<a class="reference internal" href="meanAPI.html#src.Core.means.meanLinear" title="src.Core.means.meanLinear"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.means.meanLinear()</span></tt></a>) and a
constant (<a class="reference internal" href="meanAPI.html#src.Core.means.meanConst" title="src.Core.means.meanConst"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.means.meanConst()</span></tt></a>) to get an affine function. Note, how the different components are composed using python
lists.
The hyperparameters for the mean are given in hyp.mean and consists of a single (because the input will one dimensional, i.e.
<span class="math">\(D=1\)</span>) slope (set to <span class="math">\(0.5\)</span>) and an off-set (set to <span class="math">\(1\)</span>). The number and the order of these hyperparameters conform to
the mean function specification. You can find out how many hyperparameters a mean (or covariance or likelihood function) expects by
calling it without arguments, such as <tt class="docutils literal"><span class="pre">feval(meanfunc)</span></tt>.</p>
<p>The covariance function is of the Matern form with isotropic distance measure <a class="reference internal" href="kernelAPI.html#src.Core.kernels.covMatern" title="src.Core.kernels.covMatern"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.kernels.covMatern()</span></tt></a>. This covariance
function is also composite, as it takes a constant (related to the smoothness of the GP), which in this case is set to <span class="math">\(3\)</span>. The covariance function takes
two hyperparameters, a characteristic length-scale <span class="math">\(L\)</span> and the standard deviation of the signal <span class="math">\(\sigma_f\)</span>. Note, that these positive
parameters are represented in hyp.cov using their logarithms.</p>
<p>Finally, the likelihood function is specified to be Gaussian. The standard deviation of the noise <span class="math">\(\sigma_n\)</span> is set to <span class="math">\(0.1\)</span>. Again,
the representation in the hyp.lik is given in terms of its logarithm.</p>
<p>Then, we import the dataset with <span class="math">\(n=20\)</span> examples. The inputs <span class="math">\(x\)</span> were drawn from a unit Gaussian. We then evaluate the
covariance matrix <span class="math">\(K\)</span> and the mean vector <span class="math">\(m\)</span> by calling the corresponding functions with the hyperparameters and the input locations <span class="math">\(x\)</span>. Finally,
the targets <span class="math">\(y\)</span> are imported,
and they were drawn randomly from a Gaussian with the desired covariance and mean and adding Gaussian noise with standard
deviation
<span class="math">\(\exp(hyp.lik)\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## LOAD DATA</span>
<span class="n">demoData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;../../data/regression_data.npz&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">]</span>            <span class="c"># training data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;y&#39;</span><span class="p">]</span>            <span class="c"># training target</span>
<span class="n">xstar</span> <span class="o">=</span> <span class="n">demoData</span><span class="p">[</span><span class="s">&#39;xstar&#39;</span><span class="p">]</span>    <span class="c"># test data</span>
</pre></div>
</div>
<p>The above code is a bit special because we explicitly load the data from a file (in order to
generate the same samples as were generated in GPML); ordinarily, we would only directly call the <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> function.</p>
<div class="figure align-center">
<img alt="_images/demoR1.png" src="_images/demoR1.png" />
</div>
<p>Let&#8217;s ask the model to compute the (joint) negative log probability (density) nlml (also called marginal likelihood, or evidence) and to
generalize from the training data to other (test) inputs <span class="math">\(xstar\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## PREDICTION</span>
<span class="n">vargout</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">xstar</span><span class="p">)</span>
<span class="n">ym</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="n">ys2</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="n">m</span>  <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

<span class="c">## GET negative log marginal likelihood</span>
<span class="p">[</span><span class="n">nlml</span><span class="p">,</span> <span class="n">post</span><span class="p">]</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> function is called with an instance of the hyperparameters class, hyp, and inference method.  In this example
<a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infExact" title="src.Core.inferences.infExact"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infExact()</span></tt></a> for exact inference.  The  mean, covariance and likelihood functions, as well as the inputs and outputs of the training data are also used as arguments. With no test inputs,
<tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> returns the negative log probability of the training data, in this example nlml:math:<cite>=11.97</cite>.</p>
<p>To compute the predictions at test locations, we add the test inputs <span class="math">\(z\)</span> as a final argument, and <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> returns the mean <span class="math">\(m\)</span>,
and the variance <span class="math">\(\sigma^2\)</span> at the test location(s). Plotting the mean function plus/minus two standard
deviations (corresponding to a <span class="math">\(95\%\)</span> confidence interval):</p>
<div class="figure align-center">
<img alt="_images/demoR2.png" src="_images/demoR2.png" />
</div>
<p>Typically, we would not <em>a priori</em> know the values of the hyperparameters hyp, let alone the form of the mean, covariance or likelihood
functions. So, let&#8217;s pretend we didn&#8217;t know any of this. We assume a particular structure and learn suitable hyperparameters:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">covfunc</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[</span><span class="s">&#39;kernels.covSEiso&#39;</span><span class="p">]</span> <span class="p">]</span>

<span class="c">### SET (hyper)parameters</span>
<span class="n">hyp2</span> <span class="o">=</span> <span class="n">hyperParameters</span><span class="p">()</span>
<span class="n">hyp2</span><span class="o">.</span><span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">])</span>
<span class="n">hyp2</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">hyp2</span><span class="o">.</span><span class="n">lik</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)])</span>

<span class="c">## PREDICTION</span>
<span class="n">vargout</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp2</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">xstar</span><span class="p">)</span>
<span class="n">ym</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="n">ys2</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="n">m</span>  <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

<span class="c">## GET negative log marginal likelihood</span>
<span class="p">[</span><span class="n">nlml</span><span class="p">,</span> <span class="n">post</span><span class="p">]</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp2</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<p>First, we guess that a squared exponential covariance function <a class="reference internal" href="kernelAPI.html#src.Core.kernels.covSEiso" title="src.Core.kernels.covSEiso"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.kernels.covSEiso()</span></tt></a> may be suitable. This covariance
function takes two hyperparameters: a characteristic length-scale and a signal standard deviation (magnitude). These hyperparameters are
non-negative and represented by their logarithms; thus, initializing hyp2.cov to zero, correspond to unit characteristic length-scale
and unit signal standard deviation. The likelihood hyperparameter in hyp2.lik is also initialized. We assume that the mean function is
zero, so we simply ignore it (and when in the following we call <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt>, we give an empty list for the mean function).</p>
<p>In the following line, we optimize over the hyperparameters, by minimizing the negative log marginal likelihood w.r.t. the
hyperparameters. The third parameter in the call to <tt class="xref py py-func docutils literal"><span class="pre">src.Tools.minimize()</span></tt> limits the number of function evaluations (to a maximum of
<span class="math">\(100\)</span>). The
inferred noise standard deviation is <span class="math">\(\exp(hyp2.lik)=0.15\)</span>, somewhat larger than the one used to generate the data <span class="math">\((0.1)\)</span>. The final
negative log marginal likelihood is nlml2:math:<cite>=14.13</cite>, showing that the joint probability (density) of the training data is about
<span class="math">\(\exp(14.13-11.97)=8.7\)</span> times smaller than for the setup actually generating the data. Finally, we plot the predictive distribution.</p>
<div class="figure align-center">
<img alt="_images/demoR3.png" src="_images/demoR3.png" />
</div>
<p>This plot shows clearly, that the model is indeed quite different from the generating process. This is due to the different
specifications of both the mean and covariance functions. Below we&#8217;ll try to do a better job, by allowing more flexibility in
the specification.</p>
<p>Note that the confidence interval in this plot is the confidence for the distribution of the (noisy) data. If instead you want
the confidence region for the underlying function, you should use the 3rd and 4th output arguments from <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> as these refer to the
latent process, rather than the data points:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## TRAINING: OPTIMIZE HYPERPARAMETERS</span>
<span class="c">## -&gt; parameter training via off-the-shelf optimization</span>
<span class="p">[</span><span class="n">hyp2_opt</span><span class="p">,</span> <span class="n">fopt</span><span class="p">,</span> <span class="n">gopt</span><span class="p">,</span> <span class="n">funcCalls</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_wrapper</span><span class="p">(</span><span class="n">hyp2</span><span class="p">,</span><span class="n">gp</span><span class="p">,</span><span class="s">&#39;Minimize&#39;</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">True</span><span class="p">)</span>

<span class="c">## PREDICTION</span>
<span class="n">vargout</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp2_opt</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">xstar</span><span class="p">)</span>
<span class="n">ym</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="n">ys2</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="n">m</span>  <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="n">s2</span>  <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>Here, we have changed the specification by adding the affine mean function. All the hyperparameters are learnt by optimizing
the marginal likelihood.</p>
<div class="figure align-center">
<img alt="_images/demoR4.png" src="_images/demoR4.png" />
</div>
<p>This shows that a much better fit is achieved when allowing a mean function (although the covariance function is still different
from that of the generating process).</p>
</div>
<div class="section" id="large-scale-regression">
<h1>Large scale regression<a class="headerlink" href="#large-scale-regression" title="Permalink to this headline">¶</a></h1>
<p>In case the number of training inputs <span class="math">\(x\)</span> exceeds a few thousands, exact inference using <a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infExact" title="src.Core.inferences.infExact"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infExact()</span></tt></a> takes
too long.
Instead, the FITC approximation based on a low-rank plus diagonal approximation to the exact covariance is used to deal with these cases. The general idea is
to use inducing points <span class="math">\(u\)</span> and to base the computations on cross-covariances between training, test and inducing points only.</p>
<p>Using the FITC approximation is very simple, we just have to wrap the covariance function covfunc into <a class="reference internal" href="kernelAPI.html#src.Core.kernels.covFITC" title="src.Core.kernels.covFITC"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.kernels.covFITC()</span></tt></a>
and call <tt class="xref py py-func docutils literal"><span class="pre">src.Core.gp()</span></tt> with the inference method <a class="reference internal" href="inferenceAPI.html#src.Core.inferences.infFITC" title="src.Core.inferences.infFITC"><tt class="xref py py-func docutils literal"><span class="pre">src.Core.inferences.infFITC()</span></tt></a> as demonstrated by the following lines of
code:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">## SPECIFY inducing points</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fix</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.3</span><span class="p">,</span><span class="mf">1.3</span><span class="p">,</span><span class="n">num_u</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">u</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u</span><span class="p">,(</span><span class="n">num_u</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c">## SPECIFY FITC covariance function</span>
<span class="n">covfunc</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;kernels.covFITC&#39;</span><span class="p">],</span> <span class="n">covfunc</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span>

<span class="c">## SPECIFY FICT inference method</span>
<span class="n">inffunc</span>  <span class="o">=</span> <span class="p">[</span><span class="s">&#39;inferences.infFITC&#39;</span><span class="p">]</span>

<span class="c">## TRAINING: OPTIMIZE hyperparameters</span>
<span class="p">[</span><span class="n">hyp2_opt</span><span class="p">,</span> <span class="n">fopt</span><span class="p">,</span> <span class="n">gopt</span><span class="p">,</span> <span class="n">funcCalls</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_wrapper</span><span class="p">(</span><span class="n">hyp2_opt</span><span class="p">,</span><span class="n">gp</span><span class="p">,</span><span class="s">&#39;Minimize&#39;</span><span class="p">,</span><span class="n">inffunc</span><span class="p">,</span><span class="n">meanfunc</span><span class="p">,</span><span class="n">covfunc</span><span class="p">,</span><span class="n">likfunc</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,</span><span class="bp">True</span><span class="p">)</span>
<span class="c">#[hyp2_opt, fopt, gopt, funcCalls] = min_wrapper(hyp2_opt,gp,&#39;SCG&#39;,inffunc,meanfunc,covfunc,likfunc,x,y,None,None,True)</span>
<span class="k">print</span> <span class="s">&#39;Optimal F =&#39;</span><span class="p">,</span> <span class="n">fopt</span>

<span class="c">## FITC PREDICTION</span>
<span class="n">vargout</span> <span class="o">=</span> <span class="n">gp</span><span class="p">(</span><span class="n">hyp2_opt</span><span class="p">,</span> <span class="n">inffunc</span><span class="p">,</span> <span class="n">meanfunc</span><span class="p">,</span> <span class="n">covfunc</span><span class="p">,</span> <span class="n">likfunc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xstar</span><span class="p">)</span>
<span class="n">ymF</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="n">y2F</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="n">mF</span>  <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>  <span class="n">s2F</span> <span class="o">=</span> <span class="n">vargout</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>We define equi-spaced inducing points <span class="math">\(u\)</span> that are shown in the figure as black X&#8217;s. Note that the predictive variance is
overestimated outside the support of the inducing inputs. In a multivariate example where densely sampled inducing inputs
are infeasible, one can simply use a random subset of the training points.</p>
<div class="figure align-center">
<img alt="_images/demoR5.png" src="_images/demoR5.png" />
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Regression</a></li>
<li><a class="reference internal" href="#large-scale-regression">Large scale regression</a></li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="Examples.html"
                        title="previous chapter">Demos</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="demoClassification.html"
                        title="next chapter">Classification Demonstration Example</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/demoRegression.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="demoClassification.html" title="Classification Demonstration Example"
             >next</a> |</li>
        <li class="right" >
          <a href="Examples.html" title="Demos"
             >previous</a> |</li>
        <li><a href="index.html">Procedural Gaussian Processes 1.0 documentation</a> &raquo;</li>
          <li><a href="GettingStarted.html" >Getting Started</a> &raquo;</li>
          <li><a href="Examples.html" >Demos</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Marion Neumann, Daniel Marthaler, Shan Huang, Kristian Kersting.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>